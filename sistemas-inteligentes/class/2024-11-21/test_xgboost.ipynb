{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import xgboost as xgb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0011806</th>\n",
       "      <th>0.0012137</th>\n",
       "      <th>0.001208</th>\n",
       "      <th>0.0012018</th>\n",
       "      <th>1071.2</th>\n",
       "      <th>1046.8</th>\n",
       "      <th>1048.8</th>\n",
       "      <th>1055</th>\n",
       "      <th>1180.7</th>\n",
       "      <th>1552.3</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0046729</th>\n",
       "      <th>778.89</th>\n",
       "      <th>795.98</th>\n",
       "      <th>788.88</th>\n",
       "      <th>839.12</th>\n",
       "      <th>4806.8</th>\n",
       "      <th>3260.5</th>\n",
       "      <th>3810</th>\n",
       "      <th>2836.1</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>1039.50</td>\n",
       "      <td>1033.20</td>\n",
       "      <td>1037.90</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>2061.90</td>\n",
       "      <td>2842.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>690.72</td>\n",
       "      <td>744.43</td>\n",
       "      <td>709.54</td>\n",
       "      <td>766.11</td>\n",
       "      <td>5627.8</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>5336.2</td>\n",
       "      <td>3671.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>1474.20</td>\n",
       "      <td>1506.50</td>\n",
       "      <td>1512.10</td>\n",
       "      <td>1509.20</td>\n",
       "      <td>3152.70</td>\n",
       "      <td>4917.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>871.63</td>\n",
       "      <td>1030.40</td>\n",
       "      <td>1000.50</td>\n",
       "      <td>1020.10</td>\n",
       "      <td>13398.0</td>\n",
       "      <td>7915.7</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>8232.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>1173.50</td>\n",
       "      <td>1181.10</td>\n",
       "      <td>1182.60</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>989.97</td>\n",
       "      <td>1348.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>761.77</td>\n",
       "      <td>836.36</td>\n",
       "      <td>800.78</td>\n",
       "      <td>846.97</td>\n",
       "      <td>7517.2</td>\n",
       "      <td>4942.6</td>\n",
       "      <td>6486.0</td>\n",
       "      <td>4416.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>998.09</td>\n",
       "      <td>1001.80</td>\n",
       "      <td>989.75</td>\n",
       "      <td>984.42</td>\n",
       "      <td>930.80</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>641.78</td>\n",
       "      <td>785.93</td>\n",
       "      <td>751.50</td>\n",
       "      <td>760.29</td>\n",
       "      <td>6421.8</td>\n",
       "      <td>2951.4</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>2882.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>907.20</td>\n",
       "      <td>910.03</td>\n",
       "      <td>921.85</td>\n",
       "      <td>900.22</td>\n",
       "      <td>955.55</td>\n",
       "      <td>1267.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>665.58</td>\n",
       "      <td>725.56</td>\n",
       "      <td>684.40</td>\n",
       "      <td>712.31</td>\n",
       "      <td>4015.4</td>\n",
       "      <td>2846.9</td>\n",
       "      <td>3709.8</td>\n",
       "      <td>2642.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>771.82</td>\n",
       "      <td>765.06</td>\n",
       "      <td>760.72</td>\n",
       "      <td>767.49</td>\n",
       "      <td>1757.30</td>\n",
       "      <td>2667.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>449.22</td>\n",
       "      <td>505.74</td>\n",
       "      <td>481.86</td>\n",
       "      <td>525.14</td>\n",
       "      <td>6456.4</td>\n",
       "      <td>4333.4</td>\n",
       "      <td>5510.5</td>\n",
       "      <td>3652.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>717.41</td>\n",
       "      <td>706.27</td>\n",
       "      <td>692.82</td>\n",
       "      <td>697.34</td>\n",
       "      <td>1442.90</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>605.75</td>\n",
       "      <td>603.61</td>\n",
       "      <td>527.33</td>\n",
       "      <td>604.96</td>\n",
       "      <td>1930.2</td>\n",
       "      <td>1742.7</td>\n",
       "      <td>2547.4</td>\n",
       "      <td>1486.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1184.30</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>1198.60</td>\n",
       "      <td>1190.80</td>\n",
       "      <td>2033.90</td>\n",
       "      <td>2822.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>829.15</td>\n",
       "      <td>925.68</td>\n",
       "      <td>873.47</td>\n",
       "      <td>902.69</td>\n",
       "      <td>6960.8</td>\n",
       "      <td>4208.7</td>\n",
       "      <td>5503.9</td>\n",
       "      <td>4354.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>888.70</td>\n",
       "      <td>883.39</td>\n",
       "      <td>877.48</td>\n",
       "      <td>866.00</td>\n",
       "      <td>1548.80</td>\n",
       "      <td>2111.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>682.09</td>\n",
       "      <td>734.07</td>\n",
       "      <td>688.15</td>\n",
       "      <td>697.72</td>\n",
       "      <td>3022.3</td>\n",
       "      <td>2036.6</td>\n",
       "      <td>2609.3</td>\n",
       "      <td>2124.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>1223.90</td>\n",
       "      <td>1234.90</td>\n",
       "      <td>1235.60</td>\n",
       "      <td>1238.50</td>\n",
       "      <td>2221.90</td>\n",
       "      <td>3128.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>892.92</td>\n",
       "      <td>987.35</td>\n",
       "      <td>924.71</td>\n",
       "      <td>951.54</td>\n",
       "      <td>5850.2</td>\n",
       "      <td>3591.9</td>\n",
       "      <td>4988.3</td>\n",
       "      <td>4083.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0011806  0.0012137  0.001208  0.0012018   1071.2   1046.8   1048.8  \\\n",
       "0     0.001107   0.001119  0.001116   0.001109  1039.50  1033.20  1037.90   \n",
       "1     0.000838   0.000817  0.000813   0.000816  1474.20  1506.50  1512.10   \n",
       "2     0.000959   0.000957  0.000959   0.000958  1173.50  1181.10  1182.60   \n",
       "3     0.001199   0.001197  0.001212   0.001218   998.09  1001.80   989.75   \n",
       "4     0.001475   0.001478  0.001462   0.001484   907.20   910.03   921.85   \n",
       "..         ...        ...       ...        ...      ...      ...      ...   \n",
       "911   0.001524   0.001544  0.001554   0.001533   771.82   765.06   760.72   \n",
       "912   0.002081   0.002125  0.002143   0.002136   717.41   706.27   692.82   \n",
       "913   0.001254   0.001253  0.001270   0.001281  1184.30  1209.10  1198.60   \n",
       "914   0.001481   0.001492  0.001491   0.001518   888.70   883.39   877.48   \n",
       "915   0.001311   0.001322  0.001324   0.001325  1223.90  1234.90  1235.60   \n",
       "\n",
       "        1055   1180.7  1552.3  ...  0.0046729  778.89   795.98   788.88  \\\n",
       "0    1042.00  2061.90  2842.8  ...   0.004115  690.72   744.43   709.54   \n",
       "1    1509.20  3152.70  4917.1  ...   0.003754  871.63  1030.40  1000.50   \n",
       "2    1181.20   989.97  1348.5  ...   0.003669  761.77   836.36   800.78   \n",
       "3     984.42   930.80  1843.0  ...   0.004436  641.78   785.93   751.50   \n",
       "4     900.22   955.55  1267.9  ...   0.008193  665.58   725.56   684.40   \n",
       "..       ...      ...     ...  ...        ...     ...      ...      ...   \n",
       "911   767.49  1757.30  2667.9  ...   0.009052  449.22   505.74   481.86   \n",
       "912   697.34  1442.90  1588.2  ...   0.008687  605.75   603.61   527.33   \n",
       "913  1190.80  2033.90  2822.1  ...   0.004550  829.15   925.68   873.47   \n",
       "914   866.00  1548.80  2111.8  ...   0.004107  682.09   734.07   688.15   \n",
       "915  1238.50  2221.90  3128.7  ...   0.004696  892.92   987.35   924.71   \n",
       "\n",
       "      839.12   4806.8  3260.5     3810  2836.1  -1  \n",
       "0     766.11   5627.8  3720.7   5336.2  3671.1  -1  \n",
       "1    1020.10  13398.0  7915.7  11760.0  8232.1  -1  \n",
       "2     846.97   7517.2  4942.6   6486.0  4416.6  -1  \n",
       "3     760.29   6421.8  2951.4   3344.0  2882.6  -1  \n",
       "4     712.31   4015.4  2846.9   3709.8  2642.7  -1  \n",
       "..       ...      ...     ...      ...     ...  ..  \n",
       "911   525.14   6456.4  4333.4   5510.5  3652.4  -1  \n",
       "912   604.96   1930.2  1742.7   2547.4  1486.1  -1  \n",
       "913   902.69   6960.8  4208.7   5503.9  4354.0  -1  \n",
       "914   697.72   3022.3  2036.6   2609.3  2124.5  -1  \n",
       "915   951.54   5850.2  3591.9   4988.3  4083.6  -1  \n",
       "\n",
       "[916 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"GLRLM_cyto.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trocando a classe saúdavel de -1 para 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "911    0\n",
       "912    0\n",
       "913    0\n",
       "914    0\n",
       "915    0\n",
       "Name: -1, Length: 916, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, -1]\n",
    "y = y.map({-1: 0, 1: 1})\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1\n",
       "0    674\n",
       "1    242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0011806</th>\n",
       "      <th>0.0012137</th>\n",
       "      <th>0.001208</th>\n",
       "      <th>0.0012018</th>\n",
       "      <th>1071.2</th>\n",
       "      <th>1046.8</th>\n",
       "      <th>1048.8</th>\n",
       "      <th>1055</th>\n",
       "      <th>1180.7</th>\n",
       "      <th>1552.3</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0061704</th>\n",
       "      <th>0.0046729</th>\n",
       "      <th>778.89</th>\n",
       "      <th>795.98</th>\n",
       "      <th>788.88</th>\n",
       "      <th>839.12</th>\n",
       "      <th>4806.8</th>\n",
       "      <th>3260.5</th>\n",
       "      <th>3810</th>\n",
       "      <th>2836.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>1039.50</td>\n",
       "      <td>1033.20</td>\n",
       "      <td>1037.90</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>2061.90</td>\n",
       "      <td>2842.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>690.72</td>\n",
       "      <td>744.43</td>\n",
       "      <td>709.54</td>\n",
       "      <td>766.11</td>\n",
       "      <td>5627.8</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>5336.2</td>\n",
       "      <td>3671.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>1474.20</td>\n",
       "      <td>1506.50</td>\n",
       "      <td>1512.10</td>\n",
       "      <td>1509.20</td>\n",
       "      <td>3152.70</td>\n",
       "      <td>4917.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>871.63</td>\n",
       "      <td>1030.40</td>\n",
       "      <td>1000.50</td>\n",
       "      <td>1020.10</td>\n",
       "      <td>13398.0</td>\n",
       "      <td>7915.7</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>8232.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>1173.50</td>\n",
       "      <td>1181.10</td>\n",
       "      <td>1182.60</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>989.97</td>\n",
       "      <td>1348.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>761.77</td>\n",
       "      <td>836.36</td>\n",
       "      <td>800.78</td>\n",
       "      <td>846.97</td>\n",
       "      <td>7517.2</td>\n",
       "      <td>4942.6</td>\n",
       "      <td>6486.0</td>\n",
       "      <td>4416.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>998.09</td>\n",
       "      <td>1001.80</td>\n",
       "      <td>989.75</td>\n",
       "      <td>984.42</td>\n",
       "      <td>930.80</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>641.78</td>\n",
       "      <td>785.93</td>\n",
       "      <td>751.50</td>\n",
       "      <td>760.29</td>\n",
       "      <td>6421.8</td>\n",
       "      <td>2951.4</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>2882.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>907.20</td>\n",
       "      <td>910.03</td>\n",
       "      <td>921.85</td>\n",
       "      <td>900.22</td>\n",
       "      <td>955.55</td>\n",
       "      <td>1267.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>665.58</td>\n",
       "      <td>725.56</td>\n",
       "      <td>684.40</td>\n",
       "      <td>712.31</td>\n",
       "      <td>4015.4</td>\n",
       "      <td>2846.9</td>\n",
       "      <td>3709.8</td>\n",
       "      <td>2642.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>771.82</td>\n",
       "      <td>765.06</td>\n",
       "      <td>760.72</td>\n",
       "      <td>767.49</td>\n",
       "      <td>1757.30</td>\n",
       "      <td>2667.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>449.22</td>\n",
       "      <td>505.74</td>\n",
       "      <td>481.86</td>\n",
       "      <td>525.14</td>\n",
       "      <td>6456.4</td>\n",
       "      <td>4333.4</td>\n",
       "      <td>5510.5</td>\n",
       "      <td>3652.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>717.41</td>\n",
       "      <td>706.27</td>\n",
       "      <td>692.82</td>\n",
       "      <td>697.34</td>\n",
       "      <td>1442.90</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>605.75</td>\n",
       "      <td>603.61</td>\n",
       "      <td>527.33</td>\n",
       "      <td>604.96</td>\n",
       "      <td>1930.2</td>\n",
       "      <td>1742.7</td>\n",
       "      <td>2547.4</td>\n",
       "      <td>1486.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1184.30</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>1198.60</td>\n",
       "      <td>1190.80</td>\n",
       "      <td>2033.90</td>\n",
       "      <td>2822.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>829.15</td>\n",
       "      <td>925.68</td>\n",
       "      <td>873.47</td>\n",
       "      <td>902.69</td>\n",
       "      <td>6960.8</td>\n",
       "      <td>4208.7</td>\n",
       "      <td>5503.9</td>\n",
       "      <td>4354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>888.70</td>\n",
       "      <td>883.39</td>\n",
       "      <td>877.48</td>\n",
       "      <td>866.00</td>\n",
       "      <td>1548.80</td>\n",
       "      <td>2111.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>682.09</td>\n",
       "      <td>734.07</td>\n",
       "      <td>688.15</td>\n",
       "      <td>697.72</td>\n",
       "      <td>3022.3</td>\n",
       "      <td>2036.6</td>\n",
       "      <td>2609.3</td>\n",
       "      <td>2124.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>1223.90</td>\n",
       "      <td>1234.90</td>\n",
       "      <td>1235.60</td>\n",
       "      <td>1238.50</td>\n",
       "      <td>2221.90</td>\n",
       "      <td>3128.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>892.92</td>\n",
       "      <td>987.35</td>\n",
       "      <td>924.71</td>\n",
       "      <td>951.54</td>\n",
       "      <td>5850.2</td>\n",
       "      <td>3591.9</td>\n",
       "      <td>4988.3</td>\n",
       "      <td>4083.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0011806  0.0012137  0.001208  0.0012018   1071.2   1046.8   1048.8  \\\n",
       "0     0.001107   0.001119  0.001116   0.001109  1039.50  1033.20  1037.90   \n",
       "1     0.000838   0.000817  0.000813   0.000816  1474.20  1506.50  1512.10   \n",
       "2     0.000959   0.000957  0.000959   0.000958  1173.50  1181.10  1182.60   \n",
       "3     0.001199   0.001197  0.001212   0.001218   998.09  1001.80   989.75   \n",
       "4     0.001475   0.001478  0.001462   0.001484   907.20   910.03   921.85   \n",
       "..         ...        ...       ...        ...      ...      ...      ...   \n",
       "911   0.001524   0.001544  0.001554   0.001533   771.82   765.06   760.72   \n",
       "912   0.002081   0.002125  0.002143   0.002136   717.41   706.27   692.82   \n",
       "913   0.001254   0.001253  0.001270   0.001281  1184.30  1209.10  1198.60   \n",
       "914   0.001481   0.001492  0.001491   0.001518   888.70   883.39   877.48   \n",
       "915   0.001311   0.001322  0.001324   0.001325  1223.90  1234.90  1235.60   \n",
       "\n",
       "        1055   1180.7  1552.3  ...  0.0061704  0.0046729  778.89   795.98  \\\n",
       "0    1042.00  2061.90  2842.8  ...   0.005960   0.004115  690.72   744.43   \n",
       "1    1509.20  3152.70  4917.1  ...   0.005271   0.003754  871.63  1030.40   \n",
       "2    1181.20   989.97  1348.5  ...   0.005446   0.003669  761.77   836.36   \n",
       "3     984.42   930.80  1843.0  ...   0.005279   0.004436  641.78   785.93   \n",
       "4     900.22   955.55  1267.9  ...   0.012025   0.008193  665.58   725.56   \n",
       "..       ...      ...     ...  ...        ...        ...     ...      ...   \n",
       "911   767.49  1757.30  2667.9  ...   0.013590   0.009052  449.22   505.74   \n",
       "912   697.34  1442.90  1588.2  ...   0.015723   0.008687  605.75   603.61   \n",
       "913  1190.80  2033.90  2822.1  ...   0.006287   0.004550  829.15   925.68   \n",
       "914   866.00  1548.80  2111.8  ...   0.005301   0.004107  682.09   734.07   \n",
       "915  1238.50  2221.90  3128.7  ...   0.005699   0.004696  892.92   987.35   \n",
       "\n",
       "      788.88   839.12   4806.8  3260.5     3810  2836.1  \n",
       "0     709.54   766.11   5627.8  3720.7   5336.2  3671.1  \n",
       "1    1000.50  1020.10  13398.0  7915.7  11760.0  8232.1  \n",
       "2     800.78   846.97   7517.2  4942.6   6486.0  4416.6  \n",
       "3     751.50   760.29   6421.8  2951.4   3344.0  2882.6  \n",
       "4     684.40   712.31   4015.4  2846.9   3709.8  2642.7  \n",
       "..       ...      ...      ...     ...      ...     ...  \n",
       "911   481.86   525.14   6456.4  4333.4   5510.5  3652.4  \n",
       "912   527.33   604.96   1930.2  1742.7   2547.4  1486.1  \n",
       "913   873.47   902.69   6960.8  4208.7   5503.9  4354.0  \n",
       "914   688.15   697.72   3022.3  2036.6   2609.3  2124.5  \n",
       "915   924.71   951.54   5850.2  3591.9   4988.3  4083.6  \n",
       "\n",
       "[916 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=df.columns[-1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testando com Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Kappa: 0.7027833001988071\n",
      "Fold Kappa: 0.5293768545994065\n",
      "Fold Kappa: 0.5459057071960298\n",
      "Fold Kappa: 0.5125055498002072\n",
      "Fold Kappa: 0.4851437590063816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nresultados por teste\\n\\nnormal\\n\\nFold Kappa: 0.5939849624060151\\nFold Kappa: 0.6218407248450166\\nFold Kappa: 0.5794535705166397\\nFold Kappa: 0.7283254156769596\\nFold Kappa: 0.5813725490196079\\n\\nsmote\\n\\nFold Kappa: 0.7266187050359711\\nFold Kappa: 0.5768628328485119\\nFold Kappa: 0.5696212797015652\\nFold Kappa: 0.6694915254237288\\nFold Kappa: 0.5991530373831775\\n\\nsmote_enn\\n\\nFold Kappa: 0.631578947368421\\nFold Kappa: 0.43423271500843175\\nFold Kappa: 0.5459739869182768\\nFold Kappa: 0.5221932114882507\\nFold Kappa: 0.5574679943100995\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "\n",
    "    x_train, x_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "    #smote = SMOTE(random_state=19)\n",
    "    #x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "    smote_enn = SMOTEENN(random_state=19) \n",
    "    x_train, y_train = smote_enn.fit_resample(x_train, y_train) #aplicando o balanceamento dos dados no treinamento\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators=1000,max_depth=3,colsample_bytree=1.0,learning_rate=0.2,subsample=1.0, scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train))\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Fold Kappa: {kappa}')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "resultados por teste\n",
    "\n",
    "normal\n",
    "\n",
    "Fold Kappa: 0.5939849624060151\n",
    "Fold Kappa: 0.6218407248450166\n",
    "Fold Kappa: 0.5794535705166397\n",
    "Fold Kappa: 0.7283254156769596\n",
    "Fold Kappa: 0.5813725490196079\n",
    "\n",
    "smote\n",
    "\n",
    "Fold Kappa: 0.7266187050359711\n",
    "Fold Kappa: 0.5768628328485119\n",
    "Fold Kappa: 0.5696212797015652\n",
    "Fold Kappa: 0.6694915254237288\n",
    "Fold Kappa: 0.5991530373831775\n",
    "\n",
    "smote_enn\n",
    "\n",
    "Fold Kappa: 0.631578947368421\n",
    "Fold Kappa: 0.43423271500843175\n",
    "Fold Kappa: 0.5459739869182768\n",
    "Fold Kappa: 0.5221932114882507\n",
    "Fold Kappa: 0.5574679943100995\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Kappa: 0.6983606557377049\n",
      "Fold Kappa: 0.5936135998162186\n",
      "Fold Kappa: 0.6274292059966685\n",
      "Fold Kappa: 0.617698671346202\n",
      "Fold Kappa: 0.659217877094972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nlearning_rate = None\\nn_stimators = None\\n\\nFold Kappa: 0.6240601503759399\\nFold Kappa: 0.5400565504241281\\nFold Kappa: 0.5650911011354633\\nFold Kappa: 0.7729694227541415\\nFold Kappa: 0.5377231391040755\\n\\nlearning_rate = 0.1\\n\\nFold Kappa: 0.6642335766423357\\nFold Kappa: 0.5400565504241281\\nFold Kappa: 0.5775923295454546\\nFold Kappa: 0.7373331419549304\\nFold Kappa: 0.5685415964971371\\n\\nlearning_rate = 0.1\\nn_estimators = 1000\\n\\nFold Kappa: 0.6541353383458647\\nFold Kappa: 0.6049412329095706\\nFold Kappa: 0.5723929035049762\\nFold Kappa: 0.7729694227541415\\nFold Kappa: 0.617698671346202\\n\\nlearning_rate=0.1\\nn_estimators=1000\\nmax_depth=10\\n\\nFold Kappa: 0.6417910447761195\\nFold Kappa: 0.546414008155433\\nFold Kappa: 0.5903107369929256\\nFold Kappa: 0.7875181422351234\\nFold Kappa: 0.5932998600707877\\n\\n\\nlearning_rate=0.1, \\nn_estimators=1000, \\nmax_depth=10,\\nmin_child_weight=1,\\n\\nFold Kappa: 0.6417910447761195\\nFold Kappa: 0.546414008155433\\nFold Kappa: 0.5903107369929256\\nFold Kappa: 0.7875181422351234\\nFold Kappa: 0.5932998600707877\\n\\n\\nlearning_rate=0.1, \\nn_estimators=1000, \\nmax_depth=10,\\nmin_child_weight=1,\\ngamma=0,\\n\\nFold Kappa: 0.6417910447761195\\nFold Kappa: 0.546414008155433\\nFold Kappa: 0.5903107369929256\\nFold Kappa: 0.7875181422351234\\nFold Kappa: 0.5932998600707877\\n\\n\\nlearning_rate=0.1, \\nn_estimators=1000, \\nmax_depth=10,\\nmin_child_weight=1,\\ngamma=0,\\nsubsample=1,\\n\\nFold Kappa: 0.6417910447761195\\nFold Kappa: 0.546414008155433\\nFold Kappa: 0.5903107369929256\\nFold Kappa: 0.7875181422351234\\nFold Kappa: 0.5932998600707877\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Aumentando os dados antes de terinando para balancear\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.columns = list(X.columns) + ['target']  # coluna alvo\n",
    "\n",
    "X_smote, y_smote = SMOTE(random_state=19).fit_resample(df.drop(columns=['target']), df['target'])\n",
    "\n",
    "df_balanced = pd.concat([X_smote, y_smote], axis=1)\n",
    "\n",
    "X_balanced = df_balanced.drop(columns=['target'])\n",
    "y_balanced = df_balanced['target']\n",
    "\n",
    "best = 0\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "\n",
    "    x_train, x_test = np.array(X_balanced)[train_index], np.array(X_balanced)[test_index]\n",
    "    y_train, y_test = np.array(y_balanced)[train_index], np.array(y_balanced)[test_index]\n",
    "\n",
    "    #smote = SMOTE(random_state=19)\n",
    "    #x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        learning_rate=0.1, \n",
    "        n_estimators=1000, \n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=1,\n",
    "        scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train)\n",
    "        )\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    if kappa > best:\n",
    "        best = kappa\n",
    "\n",
    "        best_model = model\n",
    "\n",
    "    print(f'Fold Kappa: {kappa}')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "learning_rate = None\n",
    "n_stimators = None\n",
    "\n",
    "Fold Kappa: 0.6240601503759399\n",
    "Fold Kappa: 0.5400565504241281\n",
    "Fold Kappa: 0.5650911011354633\n",
    "Fold Kappa: 0.7729694227541415\n",
    "Fold Kappa: 0.5377231391040755\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "Fold Kappa: 0.6642335766423357\n",
    "Fold Kappa: 0.5400565504241281\n",
    "Fold Kappa: 0.5775923295454546\n",
    "Fold Kappa: 0.7373331419549304\n",
    "Fold Kappa: 0.5685415964971371\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_estimators = 1000\n",
    "\n",
    "Fold Kappa: 0.6541353383458647\n",
    "Fold Kappa: 0.6049412329095706\n",
    "Fold Kappa: 0.5723929035049762\n",
    "Fold Kappa: 0.7729694227541415\n",
    "Fold Kappa: 0.617698671346202\n",
    "\n",
    "learning_rate=0.1\n",
    "n_estimators=1000\n",
    "max_depth=10\n",
    "\n",
    "Fold Kappa: 0.6417910447761195\n",
    "Fold Kappa: 0.546414008155433\n",
    "Fold Kappa: 0.5903107369929256\n",
    "Fold Kappa: 0.7875181422351234\n",
    "Fold Kappa: 0.5932998600707877\n",
    "\n",
    "\n",
    "learning_rate=0.1, \n",
    "n_estimators=1000, \n",
    "max_depth=10,\n",
    "min_child_weight=1,\n",
    "\n",
    "Fold Kappa: 0.6417910447761195\n",
    "Fold Kappa: 0.546414008155433\n",
    "Fold Kappa: 0.5903107369929256\n",
    "Fold Kappa: 0.7875181422351234\n",
    "Fold Kappa: 0.5932998600707877\n",
    "\n",
    "\n",
    "learning_rate=0.1, \n",
    "n_estimators=1000, \n",
    "max_depth=10,\n",
    "min_child_weight=1,\n",
    "gamma=0,\n",
    "\n",
    "Fold Kappa: 0.6417910447761195\n",
    "Fold Kappa: 0.546414008155433\n",
    "Fold Kappa: 0.5903107369929256\n",
    "Fold Kappa: 0.7875181422351234\n",
    "Fold Kappa: 0.5932998600707877\n",
    "\n",
    "\n",
    "learning_rate=0.1, \n",
    "n_estimators=1000, \n",
    "max_depth=10,\n",
    "min_child_weight=1,\n",
    "gamma=0,\n",
    "subsample=1,\n",
    "\n",
    "Fold Kappa: 0.6417910447761195\n",
    "Fold Kappa: 0.546414008155433\n",
    "Fold Kappa: 0.5903107369929256\n",
    "Fold Kappa: 0.7875181422351234\n",
    "Fold Kappa: 0.5932998600707877\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xgboost_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
