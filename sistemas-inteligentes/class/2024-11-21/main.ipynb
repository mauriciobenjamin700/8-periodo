{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackthon - 21/11/2024\n",
    "\n",
    "## Objetivo: \n",
    "\n",
    "desafio de classificação em um conjunto de dados de características extraídas de imagens. As imagens são provenientes de um exame de microscopia com o objetivo de encontrar células saudáveis e células concerígenas;\n",
    "\n",
    "## Entrega: \n",
    "\n",
    "modelo treinado no formato da biblioteca pickle e notebook;\n",
    "Prazo: até 12:00 de 22/11/2024;\n",
    "\n",
    "## Pontuação:\n",
    "\n",
    "- 1 ponto: >= 80% de acerto;\n",
    "- 0,5 ponto extra: equipe com o melhor resultado;\n",
    "- 0,5 ponto extra: equipe com o melhor código (escrita, comentários, etc);\n",
    "\n",
    "# Regras\n",
    "\n",
    "- Pode consultar o que quiser;\n",
    "\n",
    "## Obrigatório:\n",
    "- K-fold (random_state = 19)\n",
    "- Métrica Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0011806</th>\n",
       "      <th>0.0012137</th>\n",
       "      <th>0.001208</th>\n",
       "      <th>0.0012018</th>\n",
       "      <th>1071.2</th>\n",
       "      <th>1046.8</th>\n",
       "      <th>1048.8</th>\n",
       "      <th>1055</th>\n",
       "      <th>1180.7</th>\n",
       "      <th>1552.3</th>\n",
       "      <th>1462.3</th>\n",
       "      <th>1755.1</th>\n",
       "      <th>129.13</th>\n",
       "      <th>156.47</th>\n",
       "      <th>149.55</th>\n",
       "      <th>164.05</th>\n",
       "      <th>0.60938</th>\n",
       "      <th>0.69769</th>\n",
       "      <th>0.66317</th>\n",
       "      <th>0.73551</th>\n",
       "      <th>0.6878</th>\n",
       "      <th>0.73718</th>\n",
       "      <th>0.72884</th>\n",
       "      <th>0.7633</th>\n",
       "      <th>5.531</th>\n",
       "      <th>3.7235</th>\n",
       "      <th>4.3299</th>\n",
       "      <th>3.2685</th>\n",
       "      <th>0.00076172</th>\n",
       "      <th>0.00086877</th>\n",
       "      <th>0.00085176</th>\n",
       "      <th>0.00087827</th>\n",
       "      <th>0.0079293</th>\n",
       "      <th>0.0052889</th>\n",
       "      <th>0.0061704</th>\n",
       "      <th>0.0046729</th>\n",
       "      <th>778.89</th>\n",
       "      <th>795.98</th>\n",
       "      <th>788.88</th>\n",
       "      <th>839.12</th>\n",
       "      <th>4806.8</th>\n",
       "      <th>3260.5</th>\n",
       "      <th>3810</th>\n",
       "      <th>2836.1</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>1039.50</td>\n",
       "      <td>1033.20</td>\n",
       "      <td>1037.90</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>2061.90</td>\n",
       "      <td>2842.8</td>\n",
       "      <td>2272.40</td>\n",
       "      <td>2941.2</td>\n",
       "      <td>263.080</td>\n",
       "      <td>315.760</td>\n",
       "      <td>279.550</td>\n",
       "      <td>314.640</td>\n",
       "      <td>1.01060</td>\n",
       "      <td>1.17550</td>\n",
       "      <td>0.68619</td>\n",
       "      <td>1.18670</td>\n",
       "      <td>0.66175</td>\n",
       "      <td>0.72254</td>\n",
       "      <td>0.68387</td>\n",
       "      <td>0.73179</td>\n",
       "      <td>5.4466</td>\n",
       "      <td>3.5958</td>\n",
       "      <td>5.2113</td>\n",
       "      <td>3.5777</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>690.72</td>\n",
       "      <td>744.43</td>\n",
       "      <td>709.54</td>\n",
       "      <td>766.11</td>\n",
       "      <td>5627.8</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>5336.2</td>\n",
       "      <td>3671.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>1474.20</td>\n",
       "      <td>1506.50</td>\n",
       "      <td>1512.10</td>\n",
       "      <td>1509.20</td>\n",
       "      <td>3152.70</td>\n",
       "      <td>4917.1</td>\n",
       "      <td>3974.40</td>\n",
       "      <td>4650.5</td>\n",
       "      <td>502.860</td>\n",
       "      <td>665.500</td>\n",
       "      <td>591.130</td>\n",
       "      <td>649.000</td>\n",
       "      <td>1.33720</td>\n",
       "      <td>1.64370</td>\n",
       "      <td>0.83012</td>\n",
       "      <td>1.59890</td>\n",
       "      <td>0.61612</td>\n",
       "      <td>0.69902</td>\n",
       "      <td>0.66914</td>\n",
       "      <td>0.68952</td>\n",
       "      <td>8.0724</td>\n",
       "      <td>4.8277</td>\n",
       "      <td>7.1607</td>\n",
       "      <td>5.0510</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>871.63</td>\n",
       "      <td>1030.40</td>\n",
       "      <td>1000.50</td>\n",
       "      <td>1020.10</td>\n",
       "      <td>13398.0</td>\n",
       "      <td>7915.7</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>8232.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>1173.50</td>\n",
       "      <td>1181.10</td>\n",
       "      <td>1182.60</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>989.97</td>\n",
       "      <td>1348.5</td>\n",
       "      <td>1125.30</td>\n",
       "      <td>1439.7</td>\n",
       "      <td>181.100</td>\n",
       "      <td>222.810</td>\n",
       "      <td>203.650</td>\n",
       "      <td>227.540</td>\n",
       "      <td>0.63883</td>\n",
       "      <td>0.74565</td>\n",
       "      <td>0.52334</td>\n",
       "      <td>0.76793</td>\n",
       "      <td>0.65301</td>\n",
       "      <td>0.70621</td>\n",
       "      <td>0.67653</td>\n",
       "      <td>0.71998</td>\n",
       "      <td>6.3428</td>\n",
       "      <td>4.1508</td>\n",
       "      <td>5.5416</td>\n",
       "      <td>3.7508</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>761.77</td>\n",
       "      <td>836.36</td>\n",
       "      <td>800.78</td>\n",
       "      <td>846.97</td>\n",
       "      <td>7517.2</td>\n",
       "      <td>4942.6</td>\n",
       "      <td>6486.0</td>\n",
       "      <td>4416.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>998.09</td>\n",
       "      <td>1001.80</td>\n",
       "      <td>989.75</td>\n",
       "      <td>984.42</td>\n",
       "      <td>930.80</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>1685.00</td>\n",
       "      <td>1831.8</td>\n",
       "      <td>142.710</td>\n",
       "      <td>193.000</td>\n",
       "      <td>185.760</td>\n",
       "      <td>197.960</td>\n",
       "      <td>0.37485</td>\n",
       "      <td>1.10080</td>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.11070</td>\n",
       "      <td>0.61956</td>\n",
       "      <td>0.75923</td>\n",
       "      <td>0.74056</td>\n",
       "      <td>0.75332</td>\n",
       "      <td>7.3070</td>\n",
       "      <td>3.4122</td>\n",
       "      <td>3.7966</td>\n",
       "      <td>3.2234</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>641.78</td>\n",
       "      <td>785.93</td>\n",
       "      <td>751.50</td>\n",
       "      <td>760.29</td>\n",
       "      <td>6421.8</td>\n",
       "      <td>2951.4</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>2882.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>907.20</td>\n",
       "      <td>910.03</td>\n",
       "      <td>921.85</td>\n",
       "      <td>900.22</td>\n",
       "      <td>955.55</td>\n",
       "      <td>1267.9</td>\n",
       "      <td>971.25</td>\n",
       "      <td>1291.4</td>\n",
       "      <td>84.396</td>\n",
       "      <td>96.008</td>\n",
       "      <td>85.029</td>\n",
       "      <td>99.075</td>\n",
       "      <td>0.55244</td>\n",
       "      <td>0.62579</td>\n",
       "      <td>0.51733</td>\n",
       "      <td>0.64195</td>\n",
       "      <td>0.70360</td>\n",
       "      <td>0.76095</td>\n",
       "      <td>0.70291</td>\n",
       "      <td>0.75820</td>\n",
       "      <td>6.1534</td>\n",
       "      <td>4.3360</td>\n",
       "      <td>5.7433</td>\n",
       "      <td>3.9587</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>665.58</td>\n",
       "      <td>725.56</td>\n",
       "      <td>684.40</td>\n",
       "      <td>712.31</td>\n",
       "      <td>4015.4</td>\n",
       "      <td>2846.9</td>\n",
       "      <td>3709.8</td>\n",
       "      <td>2642.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>771.82</td>\n",
       "      <td>765.06</td>\n",
       "      <td>760.72</td>\n",
       "      <td>767.49</td>\n",
       "      <td>1757.30</td>\n",
       "      <td>2667.9</td>\n",
       "      <td>2344.80</td>\n",
       "      <td>3036.5</td>\n",
       "      <td>378.780</td>\n",
       "      <td>469.910</td>\n",
       "      <td>441.180</td>\n",
       "      <td>496.620</td>\n",
       "      <td>0.93267</td>\n",
       "      <td>1.11320</td>\n",
       "      <td>0.72514</td>\n",
       "      <td>1.17860</td>\n",
       "      <td>0.55816</td>\n",
       "      <td>0.64131</td>\n",
       "      <td>0.62793</td>\n",
       "      <td>0.66679</td>\n",
       "      <td>9.2472</td>\n",
       "      <td>6.1651</td>\n",
       "      <td>7.9486</td>\n",
       "      <td>5.2714</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.015783</td>\n",
       "      <td>0.010407</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>449.22</td>\n",
       "      <td>505.74</td>\n",
       "      <td>481.86</td>\n",
       "      <td>525.14</td>\n",
       "      <td>6456.4</td>\n",
       "      <td>4333.4</td>\n",
       "      <td>5510.5</td>\n",
       "      <td>3652.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>717.41</td>\n",
       "      <td>706.27</td>\n",
       "      <td>692.82</td>\n",
       "      <td>697.34</td>\n",
       "      <td>1442.90</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>1147.20</td>\n",
       "      <td>1742.5</td>\n",
       "      <td>119.020</td>\n",
       "      <td>127.850</td>\n",
       "      <td>116.280</td>\n",
       "      <td>137.210</td>\n",
       "      <td>0.90729</td>\n",
       "      <td>0.94792</td>\n",
       "      <td>0.41054</td>\n",
       "      <td>0.99479</td>\n",
       "      <td>0.77142</td>\n",
       "      <td>0.79092</td>\n",
       "      <td>0.71963</td>\n",
       "      <td>0.80771</td>\n",
       "      <td>4.1068</td>\n",
       "      <td>3.7132</td>\n",
       "      <td>5.3864</td>\n",
       "      <td>3.0782</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>605.75</td>\n",
       "      <td>603.61</td>\n",
       "      <td>527.33</td>\n",
       "      <td>604.96</td>\n",
       "      <td>1930.2</td>\n",
       "      <td>1742.7</td>\n",
       "      <td>2547.4</td>\n",
       "      <td>1486.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1184.30</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>1198.60</td>\n",
       "      <td>1190.80</td>\n",
       "      <td>2033.90</td>\n",
       "      <td>2822.1</td>\n",
       "      <td>2347.10</td>\n",
       "      <td>2755.8</td>\n",
       "      <td>167.030</td>\n",
       "      <td>207.030</td>\n",
       "      <td>189.770</td>\n",
       "      <td>204.800</td>\n",
       "      <td>0.90604</td>\n",
       "      <td>1.06250</td>\n",
       "      <td>0.71661</td>\n",
       "      <td>1.05730</td>\n",
       "      <td>0.71121</td>\n",
       "      <td>0.77257</td>\n",
       "      <td>0.73634</td>\n",
       "      <td>0.76557</td>\n",
       "      <td>5.3405</td>\n",
       "      <td>3.4361</td>\n",
       "      <td>4.3895</td>\n",
       "      <td>3.3584</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>829.15</td>\n",
       "      <td>925.68</td>\n",
       "      <td>873.47</td>\n",
       "      <td>902.69</td>\n",
       "      <td>6960.8</td>\n",
       "      <td>4208.7</td>\n",
       "      <td>5503.9</td>\n",
       "      <td>4354.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>888.70</td>\n",
       "      <td>883.39</td>\n",
       "      <td>877.48</td>\n",
       "      <td>866.00</td>\n",
       "      <td>1548.80</td>\n",
       "      <td>2111.8</td>\n",
       "      <td>1724.70</td>\n",
       "      <td>2007.8</td>\n",
       "      <td>128.660</td>\n",
       "      <td>150.830</td>\n",
       "      <td>137.660</td>\n",
       "      <td>152.960</td>\n",
       "      <td>0.59731</td>\n",
       "      <td>0.90156</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.89297</td>\n",
       "      <td>0.74434</td>\n",
       "      <td>0.80948</td>\n",
       "      <td>0.76450</td>\n",
       "      <td>0.79392</td>\n",
       "      <td>3.8033</td>\n",
       "      <td>2.5448</td>\n",
       "      <td>3.1793</td>\n",
       "      <td>2.5302</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>682.09</td>\n",
       "      <td>734.07</td>\n",
       "      <td>688.15</td>\n",
       "      <td>697.72</td>\n",
       "      <td>3022.3</td>\n",
       "      <td>2036.6</td>\n",
       "      <td>2609.3</td>\n",
       "      <td>2124.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>1223.90</td>\n",
       "      <td>1234.90</td>\n",
       "      <td>1235.60</td>\n",
       "      <td>1238.50</td>\n",
       "      <td>2221.90</td>\n",
       "      <td>3128.7</td>\n",
       "      <td>2407.70</td>\n",
       "      <td>2708.5</td>\n",
       "      <td>162.880</td>\n",
       "      <td>204.160</td>\n",
       "      <td>181.220</td>\n",
       "      <td>196.970</td>\n",
       "      <td>0.86458</td>\n",
       "      <td>1.02010</td>\n",
       "      <td>0.79942</td>\n",
       "      <td>0.96528</td>\n",
       "      <td>0.73214</td>\n",
       "      <td>0.79780</td>\n",
       "      <td>0.74345</td>\n",
       "      <td>0.76429</td>\n",
       "      <td>4.5685</td>\n",
       "      <td>2.8173</td>\n",
       "      <td>3.9078</td>\n",
       "      <td>3.2134</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>892.92</td>\n",
       "      <td>987.35</td>\n",
       "      <td>924.71</td>\n",
       "      <td>951.54</td>\n",
       "      <td>5850.2</td>\n",
       "      <td>3591.9</td>\n",
       "      <td>4988.3</td>\n",
       "      <td>4083.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0011806  0.0012137  0.001208  0.0012018  ...  3260.5     3810  2836.1  -1\n",
       "0     0.001107   0.001119  0.001116   0.001109  ...  3720.7   5336.2  3671.1  -1\n",
       "1     0.000838   0.000817  0.000813   0.000816  ...  7915.7  11760.0  8232.1  -1\n",
       "2     0.000959   0.000957  0.000959   0.000958  ...  4942.6   6486.0  4416.6  -1\n",
       "3     0.001199   0.001197  0.001212   0.001218  ...  2951.4   3344.0  2882.6  -1\n",
       "4     0.001475   0.001478  0.001462   0.001484  ...  2846.9   3709.8  2642.7  -1\n",
       "..         ...        ...       ...        ...  ...     ...      ...     ...  ..\n",
       "911   0.001524   0.001544  0.001554   0.001533  ...  4333.4   5510.5  3652.4  -1\n",
       "912   0.002081   0.002125  0.002143   0.002136  ...  1742.7   2547.4  1486.1  -1\n",
       "913   0.001254   0.001253  0.001270   0.001281  ...  4208.7   5503.9  4354.0  -1\n",
       "914   0.001481   0.001492  0.001491   0.001518  ...  2036.6   2609.3  2124.5  -1\n",
       "915   0.001311   0.001322  0.001324   0.001325  ...  3591.9   4988.3  4083.6  -1\n",
       "\n",
       "[916 rows x 45 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"GLRLM_cyto.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -1\n",
       "1     -1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1\n",
       "      ..\n",
       "911   -1\n",
       "912   -1\n",
       "913   -1\n",
       "914   -1\n",
       "915   -1\n",
       "Name: -1, Length: 916, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0011806</th>\n",
       "      <th>0.0012137</th>\n",
       "      <th>0.001208</th>\n",
       "      <th>0.0012018</th>\n",
       "      <th>1071.2</th>\n",
       "      <th>1046.8</th>\n",
       "      <th>1048.8</th>\n",
       "      <th>1055</th>\n",
       "      <th>1180.7</th>\n",
       "      <th>1552.3</th>\n",
       "      <th>1462.3</th>\n",
       "      <th>1755.1</th>\n",
       "      <th>129.13</th>\n",
       "      <th>156.47</th>\n",
       "      <th>149.55</th>\n",
       "      <th>164.05</th>\n",
       "      <th>0.60938</th>\n",
       "      <th>0.69769</th>\n",
       "      <th>0.66317</th>\n",
       "      <th>0.73551</th>\n",
       "      <th>0.6878</th>\n",
       "      <th>0.73718</th>\n",
       "      <th>0.72884</th>\n",
       "      <th>0.7633</th>\n",
       "      <th>5.531</th>\n",
       "      <th>3.7235</th>\n",
       "      <th>4.3299</th>\n",
       "      <th>3.2685</th>\n",
       "      <th>0.00076172</th>\n",
       "      <th>0.00086877</th>\n",
       "      <th>0.00085176</th>\n",
       "      <th>0.00087827</th>\n",
       "      <th>0.0079293</th>\n",
       "      <th>0.0052889</th>\n",
       "      <th>0.0061704</th>\n",
       "      <th>0.0046729</th>\n",
       "      <th>778.89</th>\n",
       "      <th>795.98</th>\n",
       "      <th>788.88</th>\n",
       "      <th>839.12</th>\n",
       "      <th>4806.8</th>\n",
       "      <th>3260.5</th>\n",
       "      <th>3810</th>\n",
       "      <th>2836.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>1039.50</td>\n",
       "      <td>1033.20</td>\n",
       "      <td>1037.90</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>2061.90</td>\n",
       "      <td>2842.8</td>\n",
       "      <td>2272.40</td>\n",
       "      <td>2941.2</td>\n",
       "      <td>263.080</td>\n",
       "      <td>315.760</td>\n",
       "      <td>279.550</td>\n",
       "      <td>314.640</td>\n",
       "      <td>1.01060</td>\n",
       "      <td>1.17550</td>\n",
       "      <td>0.68619</td>\n",
       "      <td>1.18670</td>\n",
       "      <td>0.66175</td>\n",
       "      <td>0.72254</td>\n",
       "      <td>0.68387</td>\n",
       "      <td>0.73179</td>\n",
       "      <td>5.4466</td>\n",
       "      <td>3.5958</td>\n",
       "      <td>5.2113</td>\n",
       "      <td>3.5777</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>690.72</td>\n",
       "      <td>744.43</td>\n",
       "      <td>709.54</td>\n",
       "      <td>766.11</td>\n",
       "      <td>5627.8</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>5336.2</td>\n",
       "      <td>3671.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>1474.20</td>\n",
       "      <td>1506.50</td>\n",
       "      <td>1512.10</td>\n",
       "      <td>1509.20</td>\n",
       "      <td>3152.70</td>\n",
       "      <td>4917.1</td>\n",
       "      <td>3974.40</td>\n",
       "      <td>4650.5</td>\n",
       "      <td>502.860</td>\n",
       "      <td>665.500</td>\n",
       "      <td>591.130</td>\n",
       "      <td>649.000</td>\n",
       "      <td>1.33720</td>\n",
       "      <td>1.64370</td>\n",
       "      <td>0.83012</td>\n",
       "      <td>1.59890</td>\n",
       "      <td>0.61612</td>\n",
       "      <td>0.69902</td>\n",
       "      <td>0.66914</td>\n",
       "      <td>0.68952</td>\n",
       "      <td>8.0724</td>\n",
       "      <td>4.8277</td>\n",
       "      <td>7.1607</td>\n",
       "      <td>5.0510</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>871.63</td>\n",
       "      <td>1030.40</td>\n",
       "      <td>1000.50</td>\n",
       "      <td>1020.10</td>\n",
       "      <td>13398.0</td>\n",
       "      <td>7915.7</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>8232.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>1173.50</td>\n",
       "      <td>1181.10</td>\n",
       "      <td>1182.60</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>989.97</td>\n",
       "      <td>1348.5</td>\n",
       "      <td>1125.30</td>\n",
       "      <td>1439.7</td>\n",
       "      <td>181.100</td>\n",
       "      <td>222.810</td>\n",
       "      <td>203.650</td>\n",
       "      <td>227.540</td>\n",
       "      <td>0.63883</td>\n",
       "      <td>0.74565</td>\n",
       "      <td>0.52334</td>\n",
       "      <td>0.76793</td>\n",
       "      <td>0.65301</td>\n",
       "      <td>0.70621</td>\n",
       "      <td>0.67653</td>\n",
       "      <td>0.71998</td>\n",
       "      <td>6.3428</td>\n",
       "      <td>4.1508</td>\n",
       "      <td>5.5416</td>\n",
       "      <td>3.7508</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>761.77</td>\n",
       "      <td>836.36</td>\n",
       "      <td>800.78</td>\n",
       "      <td>846.97</td>\n",
       "      <td>7517.2</td>\n",
       "      <td>4942.6</td>\n",
       "      <td>6486.0</td>\n",
       "      <td>4416.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>998.09</td>\n",
       "      <td>1001.80</td>\n",
       "      <td>989.75</td>\n",
       "      <td>984.42</td>\n",
       "      <td>930.80</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>1685.00</td>\n",
       "      <td>1831.8</td>\n",
       "      <td>142.710</td>\n",
       "      <td>193.000</td>\n",
       "      <td>185.760</td>\n",
       "      <td>197.960</td>\n",
       "      <td>0.37485</td>\n",
       "      <td>1.10080</td>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.11070</td>\n",
       "      <td>0.61956</td>\n",
       "      <td>0.75923</td>\n",
       "      <td>0.74056</td>\n",
       "      <td>0.75332</td>\n",
       "      <td>7.3070</td>\n",
       "      <td>3.4122</td>\n",
       "      <td>3.7966</td>\n",
       "      <td>3.2234</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>641.78</td>\n",
       "      <td>785.93</td>\n",
       "      <td>751.50</td>\n",
       "      <td>760.29</td>\n",
       "      <td>6421.8</td>\n",
       "      <td>2951.4</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>2882.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>907.20</td>\n",
       "      <td>910.03</td>\n",
       "      <td>921.85</td>\n",
       "      <td>900.22</td>\n",
       "      <td>955.55</td>\n",
       "      <td>1267.9</td>\n",
       "      <td>971.25</td>\n",
       "      <td>1291.4</td>\n",
       "      <td>84.396</td>\n",
       "      <td>96.008</td>\n",
       "      <td>85.029</td>\n",
       "      <td>99.075</td>\n",
       "      <td>0.55244</td>\n",
       "      <td>0.62579</td>\n",
       "      <td>0.51733</td>\n",
       "      <td>0.64195</td>\n",
       "      <td>0.70360</td>\n",
       "      <td>0.76095</td>\n",
       "      <td>0.70291</td>\n",
       "      <td>0.75820</td>\n",
       "      <td>6.1534</td>\n",
       "      <td>4.3360</td>\n",
       "      <td>5.7433</td>\n",
       "      <td>3.9587</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>665.58</td>\n",
       "      <td>725.56</td>\n",
       "      <td>684.40</td>\n",
       "      <td>712.31</td>\n",
       "      <td>4015.4</td>\n",
       "      <td>2846.9</td>\n",
       "      <td>3709.8</td>\n",
       "      <td>2642.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>771.82</td>\n",
       "      <td>765.06</td>\n",
       "      <td>760.72</td>\n",
       "      <td>767.49</td>\n",
       "      <td>1757.30</td>\n",
       "      <td>2667.9</td>\n",
       "      <td>2344.80</td>\n",
       "      <td>3036.5</td>\n",
       "      <td>378.780</td>\n",
       "      <td>469.910</td>\n",
       "      <td>441.180</td>\n",
       "      <td>496.620</td>\n",
       "      <td>0.93267</td>\n",
       "      <td>1.11320</td>\n",
       "      <td>0.72514</td>\n",
       "      <td>1.17860</td>\n",
       "      <td>0.55816</td>\n",
       "      <td>0.64131</td>\n",
       "      <td>0.62793</td>\n",
       "      <td>0.66679</td>\n",
       "      <td>9.2472</td>\n",
       "      <td>6.1651</td>\n",
       "      <td>7.9486</td>\n",
       "      <td>5.2714</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.015783</td>\n",
       "      <td>0.010407</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>449.22</td>\n",
       "      <td>505.74</td>\n",
       "      <td>481.86</td>\n",
       "      <td>525.14</td>\n",
       "      <td>6456.4</td>\n",
       "      <td>4333.4</td>\n",
       "      <td>5510.5</td>\n",
       "      <td>3652.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>717.41</td>\n",
       "      <td>706.27</td>\n",
       "      <td>692.82</td>\n",
       "      <td>697.34</td>\n",
       "      <td>1442.90</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>1147.20</td>\n",
       "      <td>1742.5</td>\n",
       "      <td>119.020</td>\n",
       "      <td>127.850</td>\n",
       "      <td>116.280</td>\n",
       "      <td>137.210</td>\n",
       "      <td>0.90729</td>\n",
       "      <td>0.94792</td>\n",
       "      <td>0.41054</td>\n",
       "      <td>0.99479</td>\n",
       "      <td>0.77142</td>\n",
       "      <td>0.79092</td>\n",
       "      <td>0.71963</td>\n",
       "      <td>0.80771</td>\n",
       "      <td>4.1068</td>\n",
       "      <td>3.7132</td>\n",
       "      <td>5.3864</td>\n",
       "      <td>3.0782</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>605.75</td>\n",
       "      <td>603.61</td>\n",
       "      <td>527.33</td>\n",
       "      <td>604.96</td>\n",
       "      <td>1930.2</td>\n",
       "      <td>1742.7</td>\n",
       "      <td>2547.4</td>\n",
       "      <td>1486.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1184.30</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>1198.60</td>\n",
       "      <td>1190.80</td>\n",
       "      <td>2033.90</td>\n",
       "      <td>2822.1</td>\n",
       "      <td>2347.10</td>\n",
       "      <td>2755.8</td>\n",
       "      <td>167.030</td>\n",
       "      <td>207.030</td>\n",
       "      <td>189.770</td>\n",
       "      <td>204.800</td>\n",
       "      <td>0.90604</td>\n",
       "      <td>1.06250</td>\n",
       "      <td>0.71661</td>\n",
       "      <td>1.05730</td>\n",
       "      <td>0.71121</td>\n",
       "      <td>0.77257</td>\n",
       "      <td>0.73634</td>\n",
       "      <td>0.76557</td>\n",
       "      <td>5.3405</td>\n",
       "      <td>3.4361</td>\n",
       "      <td>4.3895</td>\n",
       "      <td>3.3584</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>829.15</td>\n",
       "      <td>925.68</td>\n",
       "      <td>873.47</td>\n",
       "      <td>902.69</td>\n",
       "      <td>6960.8</td>\n",
       "      <td>4208.7</td>\n",
       "      <td>5503.9</td>\n",
       "      <td>4354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>888.70</td>\n",
       "      <td>883.39</td>\n",
       "      <td>877.48</td>\n",
       "      <td>866.00</td>\n",
       "      <td>1548.80</td>\n",
       "      <td>2111.8</td>\n",
       "      <td>1724.70</td>\n",
       "      <td>2007.8</td>\n",
       "      <td>128.660</td>\n",
       "      <td>150.830</td>\n",
       "      <td>137.660</td>\n",
       "      <td>152.960</td>\n",
       "      <td>0.59731</td>\n",
       "      <td>0.90156</td>\n",
       "      <td>0.82943</td>\n",
       "      <td>0.89297</td>\n",
       "      <td>0.74434</td>\n",
       "      <td>0.80948</td>\n",
       "      <td>0.76450</td>\n",
       "      <td>0.79392</td>\n",
       "      <td>3.8033</td>\n",
       "      <td>2.5448</td>\n",
       "      <td>3.1793</td>\n",
       "      <td>2.5302</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>682.09</td>\n",
       "      <td>734.07</td>\n",
       "      <td>688.15</td>\n",
       "      <td>697.72</td>\n",
       "      <td>3022.3</td>\n",
       "      <td>2036.6</td>\n",
       "      <td>2609.3</td>\n",
       "      <td>2124.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>1223.90</td>\n",
       "      <td>1234.90</td>\n",
       "      <td>1235.60</td>\n",
       "      <td>1238.50</td>\n",
       "      <td>2221.90</td>\n",
       "      <td>3128.7</td>\n",
       "      <td>2407.70</td>\n",
       "      <td>2708.5</td>\n",
       "      <td>162.880</td>\n",
       "      <td>204.160</td>\n",
       "      <td>181.220</td>\n",
       "      <td>196.970</td>\n",
       "      <td>0.86458</td>\n",
       "      <td>1.02010</td>\n",
       "      <td>0.79942</td>\n",
       "      <td>0.96528</td>\n",
       "      <td>0.73214</td>\n",
       "      <td>0.79780</td>\n",
       "      <td>0.74345</td>\n",
       "      <td>0.76429</td>\n",
       "      <td>4.5685</td>\n",
       "      <td>2.8173</td>\n",
       "      <td>3.9078</td>\n",
       "      <td>3.2134</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>892.92</td>\n",
       "      <td>987.35</td>\n",
       "      <td>924.71</td>\n",
       "      <td>951.54</td>\n",
       "      <td>5850.2</td>\n",
       "      <td>3591.9</td>\n",
       "      <td>4988.3</td>\n",
       "      <td>4083.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0011806  0.0012137  0.001208  ...  3260.5     3810  2836.1\n",
       "0     0.001107   0.001119  0.001116  ...  3720.7   5336.2  3671.1\n",
       "1     0.000838   0.000817  0.000813  ...  7915.7  11760.0  8232.1\n",
       "2     0.000959   0.000957  0.000959  ...  4942.6   6486.0  4416.6\n",
       "3     0.001199   0.001197  0.001212  ...  2951.4   3344.0  2882.6\n",
       "4     0.001475   0.001478  0.001462  ...  2846.9   3709.8  2642.7\n",
       "..         ...        ...       ...  ...     ...      ...     ...\n",
       "911   0.001524   0.001544  0.001554  ...  4333.4   5510.5  3652.4\n",
       "912   0.002081   0.002125  0.002143  ...  1742.7   2547.4  1486.1\n",
       "913   0.001254   0.001253  0.001270  ...  4208.7   5503.9  4354.0\n",
       "914   0.001481   0.001492  0.001491  ...  2036.6   2609.3  2124.5\n",
       "915   0.001311   0.001322  0.001324  ...  3591.9   4988.3  4083.6\n",
       "\n",
       "[916 rows x 44 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=df.columns[-1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    Perceptron(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    ExtraTreesClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    RidgeClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(5, shuffle=True, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - KNeighborsClassifier -> 0.5376884422110553\n",
      "Fold 1 - SVC -> 0.5767682576193214\n",
      "Fold 1 - DecisionTreeClassifier -> 0.5766500131475152\n",
      "Fold 1 - RandomForestClassifier -> 0.6377952755905512\n",
      "Fold 1 - GradientBoostingClassifier -> 0.6404243439419319\n",
      "Fold 1 - GaussianNB -> 0.6455696202531646\n",
      "Fold 1 - Perceptron -> 0.44166460242754524\n",
      "Fold 1 - MLPClassifier -> 0.29663608562691124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - AdaBoostClassifier -> 0.5746707761277668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - QuadraticDiscriminantAnalysis -> 0.7139767504731008\n",
      "Fold 1 - LinearDiscriminantAnalysis -> 0.5860517435320585\n",
      "Fold 1 - ExtraTreesClassifier -> 0.6377952755905512\n",
      "Fold 1 - BaggingClassifier -> 0.6262258335668254\n",
      "Fold 1 - RidgeClassifier -> 0.5503162737205289\n",
      "Fold 2 - KNeighborsClassifier -> 0.5250608272506083\n",
      "Fold 2 - SVC -> 0.486610558530987\n",
      "Fold 2 - DecisionTreeClassifier -> 0.5001107419712072\n",
      "Fold 2 - RandomForestClassifier -> 0.5898174831892411\n",
      "Fold 2 - GradientBoostingClassifier -> 0.600187265917603\n",
      "Fold 2 - GaussianNB -> 0.48150700666207213\n",
      "Fold 2 - Perceptron -> 0.30742358078602616\n",
      "Fold 2 - MLPClassifier -> 0.41823679925564095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - AdaBoostClassifier -> 0.5605187319884726\n",
      "Fold 2 - QuadraticDiscriminantAnalysis -> 0.5060188224994528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - LinearDiscriminantAnalysis -> 0.486610558530987\n",
      "Fold 2 - ExtraTreesClassifier -> 0.577905034597948\n",
      "Fold 2 - BaggingClassifier -> 0.5196850393700787\n",
      "Fold 2 - RidgeClassifier -> 0.5234375\n",
      "Fold 3 - KNeighborsClassifier -> 0.4801612755429304\n",
      "Fold 3 - SVC -> 0.5823280078252364\n",
      "Fold 3 - DecisionTreeClassifier -> 0.5490695895997961\n",
      "Fold 3 - RandomForestClassifier -> 0.5678866587957496\n",
      "Fold 3 - GradientBoostingClassifier -> 0.5189368123843832\n",
      "Fold 3 - GaussianNB -> 0.5507930513595166\n",
      "Fold 3 - Perceptron -> 0.2932179823883825\n",
      "Fold 3 - MLPClassifier -> 0.19049657534246578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - AdaBoostClassifier -> 0.5136992577659671\n",
      "Fold 3 - QuadraticDiscriminantAnalysis -> 0.6473308922721142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - LinearDiscriminantAnalysis -> 0.5572905212227842\n",
      "Fold 3 - ExtraTreesClassifier -> 0.5814855324649497\n",
      "Fold 3 - BaggingClassifier -> 0.5636623748211731\n",
      "Fold 3 - RidgeClassifier -> 0.5316568376543898\n",
      "Fold 4 - KNeighborsClassifier -> 0.4301094403416674\n",
      "Fold 4 - SVC -> 0.4380038387715931\n",
      "Fold 4 - DecisionTreeClassifier -> 0.46176470588235297\n",
      "Fold 4 - RandomForestClassifier -> 0.6060279870828849\n",
      "Fold 4 - GradientBoostingClassifier -> 0.5603701396921434\n",
      "Fold 4 - GaussianNB -> 0.4760862243179522\n",
      "Fold 4 - Perceptron -> 0.23966761425759897\n",
      "Fold 4 - MLPClassifier -> 0.40287907869481765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - AdaBoostClassifier -> 0.5278049648545244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - QuadraticDiscriminantAnalysis -> 0.4795311557355856\n",
      "Fold 4 - LinearDiscriminantAnalysis -> 0.4153354632587859\n",
      "Fold 4 - ExtraTreesClassifier -> 0.5531337614181062\n",
      "Fold 4 - BaggingClassifier -> 0.5793103448275863\n",
      "Fold 4 - RidgeClassifier -> 0.36877894944318523\n",
      "Fold 5 - KNeighborsClassifier -> 0.5481481481481482\n",
      "Fold 5 - SVC -> 0.6205943331029717\n",
      "Fold 5 - DecisionTreeClassifier -> 0.5339198888631627\n",
      "Fold 5 - RandomForestClassifier -> 0.6856750257643421\n",
      "Fold 5 - GradientBoostingClassifier -> 0.7171075231879079\n",
      "Fold 5 - GaussianNB -> 0.5793103448275863\n",
      "Fold 5 - Perceptron -> 0.6197007481296758\n",
      "Fold 5 - MLPClassifier -> 0.4737889847378898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - AdaBoostClassifier -> 0.6170586758181971\n",
      "Fold 5 - QuadraticDiscriminantAnalysis -> 0.6743772241992882\n",
      "Fold 5 - LinearDiscriminantAnalysis -> 0.6558209516644724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/projects/course/ufpi/8-periodo/sistemas-inteligentes/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - ExtraTreesClassifier -> 0.6611409928577727\n",
      "Fold 5 - BaggingClassifier -> 0.6104725415070242\n",
      "Fold 5 - RidgeClassifier -> 0.6842407975460123\n"
     ]
    }
   ],
   "source": [
    "kappas = {}\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "    x_train, x_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        model_name = model.__class__.__name__\n",
    "        kappa = cohen_kappa_score(y_test, y_pred)\n",
    "        print(f\"Fold {i+1} - {model_name} -> {kappa}\")\n",
    "        kappas[model_name] = kappas.get(model_name, 0) + kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier -> 0.6174404860845537\n",
      "GradientBoostingClassifier -> 0.6074052170247939\n",
      "QuadraticDiscriminantAnalysis -> 0.6042469690359084\n",
      "ExtraTreesClassifier -> 0.6022921193858656\n"
     ]
    }
   ],
   "source": [
    "for model_name, kappa in kappas.items():\n",
    "    mean_kappa = kappa/5\n",
    "    if mean_kappa > 0.6:\n",
    "        print(f\"{model_name} -> {kappa/5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
