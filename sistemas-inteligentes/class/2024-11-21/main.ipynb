{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackthon - 21/11/2024\n",
    "\n",
    "## Objetivo: \n",
    "\n",
    "desafio de classificação em um conjunto de dados de características extraídas de imagens. As imagens são provenientes de um exame de microscopia com o objetivo de encontrar células saudáveis e células concerígenas;\n",
    "\n",
    "## Entrega: \n",
    "\n",
    "modelo treinado no formato da biblioteca pickle e notebook;\n",
    "Prazo: até 12:00 de 22/11/2024;\n",
    "\n",
    "## Pontuação:\n",
    "\n",
    "- 1 ponto: >= 80% de acerto;\n",
    "- 0,5 ponto extra: equipe com o melhor resultado;\n",
    "- 0,5 ponto extra: equipe com o melhor código (escrita, comentários, etc);\n",
    "\n",
    "# Regras\n",
    "\n",
    "- Pode consultar o que quiser;\n",
    "\n",
    "## Obrigatório:\n",
    "- K-fold (random_state = 19)\n",
    "- Métrica Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0011806</th>\n",
       "      <th>0.0012137</th>\n",
       "      <th>0.001208</th>\n",
       "      <th>0.0012018</th>\n",
       "      <th>1071.2</th>\n",
       "      <th>1046.8</th>\n",
       "      <th>1048.8</th>\n",
       "      <th>1055</th>\n",
       "      <th>1180.7</th>\n",
       "      <th>1552.3</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0046729</th>\n",
       "      <th>778.89</th>\n",
       "      <th>795.98</th>\n",
       "      <th>788.88</th>\n",
       "      <th>839.12</th>\n",
       "      <th>4806.8</th>\n",
       "      <th>3260.5</th>\n",
       "      <th>3810</th>\n",
       "      <th>2836.1</th>\n",
       "      <th>-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>1039.50</td>\n",
       "      <td>1033.20</td>\n",
       "      <td>1037.90</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>2061.90</td>\n",
       "      <td>2842.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>690.72</td>\n",
       "      <td>744.43</td>\n",
       "      <td>709.54</td>\n",
       "      <td>766.11</td>\n",
       "      <td>5627.8</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>5336.2</td>\n",
       "      <td>3671.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>1474.20</td>\n",
       "      <td>1506.50</td>\n",
       "      <td>1512.10</td>\n",
       "      <td>1509.20</td>\n",
       "      <td>3152.70</td>\n",
       "      <td>4917.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>871.63</td>\n",
       "      <td>1030.40</td>\n",
       "      <td>1000.50</td>\n",
       "      <td>1020.10</td>\n",
       "      <td>13398.0</td>\n",
       "      <td>7915.7</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>8232.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>1173.50</td>\n",
       "      <td>1181.10</td>\n",
       "      <td>1182.60</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>989.97</td>\n",
       "      <td>1348.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>761.77</td>\n",
       "      <td>836.36</td>\n",
       "      <td>800.78</td>\n",
       "      <td>846.97</td>\n",
       "      <td>7517.2</td>\n",
       "      <td>4942.6</td>\n",
       "      <td>6486.0</td>\n",
       "      <td>4416.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>998.09</td>\n",
       "      <td>1001.80</td>\n",
       "      <td>989.75</td>\n",
       "      <td>984.42</td>\n",
       "      <td>930.80</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>641.78</td>\n",
       "      <td>785.93</td>\n",
       "      <td>751.50</td>\n",
       "      <td>760.29</td>\n",
       "      <td>6421.8</td>\n",
       "      <td>2951.4</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>2882.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>907.20</td>\n",
       "      <td>910.03</td>\n",
       "      <td>921.85</td>\n",
       "      <td>900.22</td>\n",
       "      <td>955.55</td>\n",
       "      <td>1267.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>665.58</td>\n",
       "      <td>725.56</td>\n",
       "      <td>684.40</td>\n",
       "      <td>712.31</td>\n",
       "      <td>4015.4</td>\n",
       "      <td>2846.9</td>\n",
       "      <td>3709.8</td>\n",
       "      <td>2642.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>771.82</td>\n",
       "      <td>765.06</td>\n",
       "      <td>760.72</td>\n",
       "      <td>767.49</td>\n",
       "      <td>1757.30</td>\n",
       "      <td>2667.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>449.22</td>\n",
       "      <td>505.74</td>\n",
       "      <td>481.86</td>\n",
       "      <td>525.14</td>\n",
       "      <td>6456.4</td>\n",
       "      <td>4333.4</td>\n",
       "      <td>5510.5</td>\n",
       "      <td>3652.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>717.41</td>\n",
       "      <td>706.27</td>\n",
       "      <td>692.82</td>\n",
       "      <td>697.34</td>\n",
       "      <td>1442.90</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>605.75</td>\n",
       "      <td>603.61</td>\n",
       "      <td>527.33</td>\n",
       "      <td>604.96</td>\n",
       "      <td>1930.2</td>\n",
       "      <td>1742.7</td>\n",
       "      <td>2547.4</td>\n",
       "      <td>1486.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1184.30</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>1198.60</td>\n",
       "      <td>1190.80</td>\n",
       "      <td>2033.90</td>\n",
       "      <td>2822.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>829.15</td>\n",
       "      <td>925.68</td>\n",
       "      <td>873.47</td>\n",
       "      <td>902.69</td>\n",
       "      <td>6960.8</td>\n",
       "      <td>4208.7</td>\n",
       "      <td>5503.9</td>\n",
       "      <td>4354.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>888.70</td>\n",
       "      <td>883.39</td>\n",
       "      <td>877.48</td>\n",
       "      <td>866.00</td>\n",
       "      <td>1548.80</td>\n",
       "      <td>2111.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>682.09</td>\n",
       "      <td>734.07</td>\n",
       "      <td>688.15</td>\n",
       "      <td>697.72</td>\n",
       "      <td>3022.3</td>\n",
       "      <td>2036.6</td>\n",
       "      <td>2609.3</td>\n",
       "      <td>2124.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>1223.90</td>\n",
       "      <td>1234.90</td>\n",
       "      <td>1235.60</td>\n",
       "      <td>1238.50</td>\n",
       "      <td>2221.90</td>\n",
       "      <td>3128.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>892.92</td>\n",
       "      <td>987.35</td>\n",
       "      <td>924.71</td>\n",
       "      <td>951.54</td>\n",
       "      <td>5850.2</td>\n",
       "      <td>3591.9</td>\n",
       "      <td>4988.3</td>\n",
       "      <td>4083.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0011806  0.0012137  0.001208  0.0012018   1071.2   1046.8   1048.8  \\\n",
       "0     0.001107   0.001119  0.001116   0.001109  1039.50  1033.20  1037.90   \n",
       "1     0.000838   0.000817  0.000813   0.000816  1474.20  1506.50  1512.10   \n",
       "2     0.000959   0.000957  0.000959   0.000958  1173.50  1181.10  1182.60   \n",
       "3     0.001199   0.001197  0.001212   0.001218   998.09  1001.80   989.75   \n",
       "4     0.001475   0.001478  0.001462   0.001484   907.20   910.03   921.85   \n",
       "..         ...        ...       ...        ...      ...      ...      ...   \n",
       "911   0.001524   0.001544  0.001554   0.001533   771.82   765.06   760.72   \n",
       "912   0.002081   0.002125  0.002143   0.002136   717.41   706.27   692.82   \n",
       "913   0.001254   0.001253  0.001270   0.001281  1184.30  1209.10  1198.60   \n",
       "914   0.001481   0.001492  0.001491   0.001518   888.70   883.39   877.48   \n",
       "915   0.001311   0.001322  0.001324   0.001325  1223.90  1234.90  1235.60   \n",
       "\n",
       "        1055   1180.7  1552.3  ...  0.0046729  778.89   795.98   788.88  \\\n",
       "0    1042.00  2061.90  2842.8  ...   0.004115  690.72   744.43   709.54   \n",
       "1    1509.20  3152.70  4917.1  ...   0.003754  871.63  1030.40  1000.50   \n",
       "2    1181.20   989.97  1348.5  ...   0.003669  761.77   836.36   800.78   \n",
       "3     984.42   930.80  1843.0  ...   0.004436  641.78   785.93   751.50   \n",
       "4     900.22   955.55  1267.9  ...   0.008193  665.58   725.56   684.40   \n",
       "..       ...      ...     ...  ...        ...     ...      ...      ...   \n",
       "911   767.49  1757.30  2667.9  ...   0.009052  449.22   505.74   481.86   \n",
       "912   697.34  1442.90  1588.2  ...   0.008687  605.75   603.61   527.33   \n",
       "913  1190.80  2033.90  2822.1  ...   0.004550  829.15   925.68   873.47   \n",
       "914   866.00  1548.80  2111.8  ...   0.004107  682.09   734.07   688.15   \n",
       "915  1238.50  2221.90  3128.7  ...   0.004696  892.92   987.35   924.71   \n",
       "\n",
       "      839.12   4806.8  3260.5     3810  2836.1  -1  \n",
       "0     766.11   5627.8  3720.7   5336.2  3671.1  -1  \n",
       "1    1020.10  13398.0  7915.7  11760.0  8232.1  -1  \n",
       "2     846.97   7517.2  4942.6   6486.0  4416.6  -1  \n",
       "3     760.29   6421.8  2951.4   3344.0  2882.6  -1  \n",
       "4     712.31   4015.4  2846.9   3709.8  2642.7  -1  \n",
       "..       ...      ...     ...      ...     ...  ..  \n",
       "911   525.14   6456.4  4333.4   5510.5  3652.4  -1  \n",
       "912   604.96   1930.2  1742.7   2547.4  1486.1  -1  \n",
       "913   902.69   6960.8  4208.7   5503.9  4354.0  -1  \n",
       "914   697.72   3022.3  2036.6   2609.3  2124.5  -1  \n",
       "915   951.54   5850.2  3591.9   4988.3  4083.6  -1  \n",
       "\n",
       "[916 rows x 45 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"GLRLM_cyto.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -1\n",
       "1     -1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1\n",
       "      ..\n",
       "911   -1\n",
       "912   -1\n",
       "913   -1\n",
       "914   -1\n",
       "915   -1\n",
       "Name: -1, Length: 916, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "911    0\n",
       "912    0\n",
       "913    0\n",
       "914    0\n",
       "915    0\n",
       "Name: -1, Length: 916, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.map({-1: 0, 1: 1})\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1\n",
       "0    674\n",
       "1    242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1\n",
       "0    674\n",
       "1    242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0011806</th>\n",
       "      <th>0.0012137</th>\n",
       "      <th>0.001208</th>\n",
       "      <th>0.0012018</th>\n",
       "      <th>1071.2</th>\n",
       "      <th>1046.8</th>\n",
       "      <th>1048.8</th>\n",
       "      <th>1055</th>\n",
       "      <th>1180.7</th>\n",
       "      <th>1552.3</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0061704</th>\n",
       "      <th>0.0046729</th>\n",
       "      <th>778.89</th>\n",
       "      <th>795.98</th>\n",
       "      <th>788.88</th>\n",
       "      <th>839.12</th>\n",
       "      <th>4806.8</th>\n",
       "      <th>3260.5</th>\n",
       "      <th>3810</th>\n",
       "      <th>2836.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>1039.50</td>\n",
       "      <td>1033.20</td>\n",
       "      <td>1037.90</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>2061.90</td>\n",
       "      <td>2842.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>690.72</td>\n",
       "      <td>744.43</td>\n",
       "      <td>709.54</td>\n",
       "      <td>766.11</td>\n",
       "      <td>5627.8</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>5336.2</td>\n",
       "      <td>3671.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>1474.20</td>\n",
       "      <td>1506.50</td>\n",
       "      <td>1512.10</td>\n",
       "      <td>1509.20</td>\n",
       "      <td>3152.70</td>\n",
       "      <td>4917.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>871.63</td>\n",
       "      <td>1030.40</td>\n",
       "      <td>1000.50</td>\n",
       "      <td>1020.10</td>\n",
       "      <td>13398.0</td>\n",
       "      <td>7915.7</td>\n",
       "      <td>11760.0</td>\n",
       "      <td>8232.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>1173.50</td>\n",
       "      <td>1181.10</td>\n",
       "      <td>1182.60</td>\n",
       "      <td>1181.20</td>\n",
       "      <td>989.97</td>\n",
       "      <td>1348.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>761.77</td>\n",
       "      <td>836.36</td>\n",
       "      <td>800.78</td>\n",
       "      <td>846.97</td>\n",
       "      <td>7517.2</td>\n",
       "      <td>4942.6</td>\n",
       "      <td>6486.0</td>\n",
       "      <td>4416.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>998.09</td>\n",
       "      <td>1001.80</td>\n",
       "      <td>989.75</td>\n",
       "      <td>984.42</td>\n",
       "      <td>930.80</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>641.78</td>\n",
       "      <td>785.93</td>\n",
       "      <td>751.50</td>\n",
       "      <td>760.29</td>\n",
       "      <td>6421.8</td>\n",
       "      <td>2951.4</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>2882.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>907.20</td>\n",
       "      <td>910.03</td>\n",
       "      <td>921.85</td>\n",
       "      <td>900.22</td>\n",
       "      <td>955.55</td>\n",
       "      <td>1267.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>665.58</td>\n",
       "      <td>725.56</td>\n",
       "      <td>684.40</td>\n",
       "      <td>712.31</td>\n",
       "      <td>4015.4</td>\n",
       "      <td>2846.9</td>\n",
       "      <td>3709.8</td>\n",
       "      <td>2642.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>771.82</td>\n",
       "      <td>765.06</td>\n",
       "      <td>760.72</td>\n",
       "      <td>767.49</td>\n",
       "      <td>1757.30</td>\n",
       "      <td>2667.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>449.22</td>\n",
       "      <td>505.74</td>\n",
       "      <td>481.86</td>\n",
       "      <td>525.14</td>\n",
       "      <td>6456.4</td>\n",
       "      <td>4333.4</td>\n",
       "      <td>5510.5</td>\n",
       "      <td>3652.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>717.41</td>\n",
       "      <td>706.27</td>\n",
       "      <td>692.82</td>\n",
       "      <td>697.34</td>\n",
       "      <td>1442.90</td>\n",
       "      <td>1588.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015723</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>605.75</td>\n",
       "      <td>603.61</td>\n",
       "      <td>527.33</td>\n",
       "      <td>604.96</td>\n",
       "      <td>1930.2</td>\n",
       "      <td>1742.7</td>\n",
       "      <td>2547.4</td>\n",
       "      <td>1486.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>1184.30</td>\n",
       "      <td>1209.10</td>\n",
       "      <td>1198.60</td>\n",
       "      <td>1190.80</td>\n",
       "      <td>2033.90</td>\n",
       "      <td>2822.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>829.15</td>\n",
       "      <td>925.68</td>\n",
       "      <td>873.47</td>\n",
       "      <td>902.69</td>\n",
       "      <td>6960.8</td>\n",
       "      <td>4208.7</td>\n",
       "      <td>5503.9</td>\n",
       "      <td>4354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>888.70</td>\n",
       "      <td>883.39</td>\n",
       "      <td>877.48</td>\n",
       "      <td>866.00</td>\n",
       "      <td>1548.80</td>\n",
       "      <td>2111.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>682.09</td>\n",
       "      <td>734.07</td>\n",
       "      <td>688.15</td>\n",
       "      <td>697.72</td>\n",
       "      <td>3022.3</td>\n",
       "      <td>2036.6</td>\n",
       "      <td>2609.3</td>\n",
       "      <td>2124.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>1223.90</td>\n",
       "      <td>1234.90</td>\n",
       "      <td>1235.60</td>\n",
       "      <td>1238.50</td>\n",
       "      <td>2221.90</td>\n",
       "      <td>3128.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>892.92</td>\n",
       "      <td>987.35</td>\n",
       "      <td>924.71</td>\n",
       "      <td>951.54</td>\n",
       "      <td>5850.2</td>\n",
       "      <td>3591.9</td>\n",
       "      <td>4988.3</td>\n",
       "      <td>4083.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0011806  0.0012137  0.001208  0.0012018   1071.2   1046.8   1048.8  \\\n",
       "0     0.001107   0.001119  0.001116   0.001109  1039.50  1033.20  1037.90   \n",
       "1     0.000838   0.000817  0.000813   0.000816  1474.20  1506.50  1512.10   \n",
       "2     0.000959   0.000957  0.000959   0.000958  1173.50  1181.10  1182.60   \n",
       "3     0.001199   0.001197  0.001212   0.001218   998.09  1001.80   989.75   \n",
       "4     0.001475   0.001478  0.001462   0.001484   907.20   910.03   921.85   \n",
       "..         ...        ...       ...        ...      ...      ...      ...   \n",
       "911   0.001524   0.001544  0.001554   0.001533   771.82   765.06   760.72   \n",
       "912   0.002081   0.002125  0.002143   0.002136   717.41   706.27   692.82   \n",
       "913   0.001254   0.001253  0.001270   0.001281  1184.30  1209.10  1198.60   \n",
       "914   0.001481   0.001492  0.001491   0.001518   888.70   883.39   877.48   \n",
       "915   0.001311   0.001322  0.001324   0.001325  1223.90  1234.90  1235.60   \n",
       "\n",
       "        1055   1180.7  1552.3  ...  0.0061704  0.0046729  778.89   795.98  \\\n",
       "0    1042.00  2061.90  2842.8  ...   0.005960   0.004115  690.72   744.43   \n",
       "1    1509.20  3152.70  4917.1  ...   0.005271   0.003754  871.63  1030.40   \n",
       "2    1181.20   989.97  1348.5  ...   0.005446   0.003669  761.77   836.36   \n",
       "3     984.42   930.80  1843.0  ...   0.005279   0.004436  641.78   785.93   \n",
       "4     900.22   955.55  1267.9  ...   0.012025   0.008193  665.58   725.56   \n",
       "..       ...      ...     ...  ...        ...        ...     ...      ...   \n",
       "911   767.49  1757.30  2667.9  ...   0.013590   0.009052  449.22   505.74   \n",
       "912   697.34  1442.90  1588.2  ...   0.015723   0.008687  605.75   603.61   \n",
       "913  1190.80  2033.90  2822.1  ...   0.006287   0.004550  829.15   925.68   \n",
       "914   866.00  1548.80  2111.8  ...   0.005301   0.004107  682.09   734.07   \n",
       "915  1238.50  2221.90  3128.7  ...   0.005699   0.004696  892.92   987.35   \n",
       "\n",
       "      788.88   839.12   4806.8  3260.5     3810  2836.1  \n",
       "0     709.54   766.11   5627.8  3720.7   5336.2  3671.1  \n",
       "1    1000.50  1020.10  13398.0  7915.7  11760.0  8232.1  \n",
       "2     800.78   846.97   7517.2  4942.6   6486.0  4416.6  \n",
       "3     751.50   760.29   6421.8  2951.4   3344.0  2882.6  \n",
       "4     684.40   712.31   4015.4  2846.9   3709.8  2642.7  \n",
       "..       ...      ...      ...     ...      ...     ...  \n",
       "911   481.86   525.14   6456.4  4333.4   5510.5  3652.4  \n",
       "912   527.33   604.96   1930.2  1742.7   2547.4  1486.1  \n",
       "913   873.47   902.69   6960.8  4208.7   5503.9  4354.0  \n",
       "914   688.15   697.72   3022.3  2036.6   2609.3  2124.5  \n",
       "915   924.71   951.54   5850.2  3591.9   4988.3  4083.6  \n",
       "\n",
       "[916 rows x 44 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=df.columns[-1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    Perceptron(),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    ExtraTreesClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    RidgeClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(5, shuffle=True, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - KNeighborsClassifier -> 0.5376884422110553\n",
      "Fold 1 - SVC -> 0.5767682576193214\n",
      "Fold 1 - DecisionTreeClassifier -> 0.5250198570293884\n",
      "Fold 1 - RandomForestClassifier -> 0.597516229184307\n",
      "Fold 1 - GradientBoostingClassifier -> 0.6570008285004143\n",
      "Fold 1 - GaussianNB -> 0.6455696202531646\n",
      "Fold 1 - Perceptron -> 0.44166460242754524\n",
      "Fold 1 - MLPClassifier -> 0.6480874316939891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - AdaBoostClassifier -> 0.5746707761277668\n",
      "Fold 1 - QuadraticDiscriminantAnalysis -> 0.7139767504731008\n",
      "Fold 1 - LinearDiscriminantAnalysis -> 0.5860517435320585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - ExtraTreesClassifier -> 0.635127478753541\n",
      "Fold 1 - BaggingClassifier -> 0.6175166297117517\n",
      "Fold 1 - RidgeClassifier -> 0.5503162737205289\n",
      "Fold 2 - KNeighborsClassifier -> 0.5250608272506083\n",
      "Fold 2 - SVC -> 0.486610558530987\n",
      "Fold 2 - DecisionTreeClassifier -> 0.5433729634522237\n",
      "Fold 2 - RandomForestClassifier -> 0.566145092460882\n",
      "Fold 2 - GradientBoostingClassifier -> 0.5832744405182568\n",
      "Fold 2 - GaussianNB -> 0.48150700666207213\n",
      "Fold 2 - Perceptron -> 0.30742358078602616\n",
      "Fold 2 - MLPClassifier -> 0.46146504414220946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - AdaBoostClassifier -> 0.5605187319884726\n",
      "Fold 2 - QuadraticDiscriminantAnalysis -> 0.5060188224994528\n",
      "Fold 2 - LinearDiscriminantAnalysis -> 0.486610558530987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - ExtraTreesClassifier -> 0.5547445255474452\n",
      "Fold 2 - BaggingClassifier -> 0.5601302628518261\n",
      "Fold 2 - RidgeClassifier -> 0.5234375\n",
      "Fold 3 - KNeighborsClassifier -> 0.4801612755429304\n",
      "Fold 3 - SVC -> 0.5823280078252364\n",
      "Fold 3 - DecisionTreeClassifier -> 0.45508684863523574\n",
      "Fold 3 - RandomForestClassifier -> 0.5545711225781327\n",
      "Fold 3 - GradientBoostingClassifier -> 0.5062632491809598\n",
      "Fold 3 - GaussianNB -> 0.5507930513595166\n",
      "Fold 3 - Perceptron -> 0.2932179823883825\n",
      "Fold 3 - MLPClassifier -> 0.528755364806867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - AdaBoostClassifier -> 0.5136992577659671\n",
      "Fold 3 - QuadraticDiscriminantAnalysis -> 0.6473308922721142\n",
      "Fold 3 - LinearDiscriminantAnalysis -> 0.5572905212227842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - ExtraTreesClassifier -> 0.5450929700705975\n",
      "Fold 3 - BaggingClassifier -> 0.6120639814993255\n",
      "Fold 3 - RidgeClassifier -> 0.5316568376543898\n",
      "Fold 4 - KNeighborsClassifier -> 0.4301094403416674\n",
      "Fold 4 - SVC -> 0.4380038387715931\n",
      "Fold 4 - DecisionTreeClassifier -> 0.4003836010549029\n",
      "Fold 4 - RandomForestClassifier -> 0.6193361671339424\n",
      "Fold 4 - GradientBoostingClassifier -> 0.5994221171526136\n",
      "Fold 4 - GaussianNB -> 0.4760862243179522\n",
      "Fold 4 - Perceptron -> 0.23966761425759897\n",
      "Fold 4 - MLPClassifier -> 0.3325540072944917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - AdaBoostClassifier -> 0.5278049648545244\n",
      "Fold 4 - QuadraticDiscriminantAnalysis -> 0.4795311557355856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - LinearDiscriminantAnalysis -> 0.4153354632587859\n",
      "Fold 4 - ExtraTreesClassifier -> 0.5661134415465985\n",
      "Fold 4 - BaggingClassifier -> 0.5353296558970317\n",
      "Fold 4 - RidgeClassifier -> 0.36877894944318523\n",
      "Fold 5 - KNeighborsClassifier -> 0.5481481481481482\n",
      "Fold 5 - SVC -> 0.6205943331029717\n",
      "Fold 5 - DecisionTreeClassifier -> 0.5003212335367813\n",
      "Fold 5 - RandomForestClassifier -> 0.6987784804643506\n",
      "Fold 5 - GradientBoostingClassifier -> 0.6987784804643506\n",
      "Fold 5 - GaussianNB -> 0.5793103448275863\n",
      "Fold 5 - Perceptron -> 0.6197007481296758\n",
      "Fold 5 - MLPClassifier -> 0.4027825489522501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - AdaBoostClassifier -> 0.6170586758181971\n",
      "Fold 5 - QuadraticDiscriminantAnalysis -> 0.6743772241992882\n",
      "Fold 5 - LinearDiscriminantAnalysis -> 0.6558209516644724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - ExtraTreesClassifier -> 0.6987784804643506\n",
      "Fold 5 - BaggingClassifier -> 0.5841636077608809\n",
      "Fold 5 - RidgeClassifier -> 0.6842407975460123\n"
     ]
    }
   ],
   "source": [
    "def kappa_cross_val(models: list[RandomForestClassifier, GradientBoostingClassifier], X: pd.Series, y: pd.Series):\n",
    "    kappas = {}\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        x_train, x_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "        for model in models:\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            model_name = model.__class__.__name__\n",
    "            kappa = cohen_kappa_score(y_test, y_pred)\n",
    "            print(f\"Fold {i+1} - {model_name} -> {kappa}\")\n",
    "            kappas[model_name] = kappas.get(model_name, 0) + kappa\n",
    "\n",
    "    return kappas\n",
    "\n",
    "kappas = kappa_cross_val(models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier -> 0.6072694183643229\n",
      "GradientBoostingClassifier -> 0.6089478231633191\n",
      "QuadraticDiscriminantAnalysis -> 0.6042469690359084\n"
     ]
    }
   ],
   "source": [
    "for model_name, kappa in kappas.items():\n",
    "    mean_kappa = kappa/5\n",
    "    if mean_kappa > 0.6:\n",
    "        print(f\"{model_name} -> {kappa/5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', bootstrap=True, criterion='gini', random_state=42, class_weight='balanced')\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "qda = QuadraticDiscriminantAnalysis(reg_param=0.0, store_covariance=False, tol=1e-4)\n",
    "etc = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rf, gbc, qda, etc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - RandomForestClassifier -> 0.6377952755905512\n",
      "Fold 1 - GradientBoostingClassifier -> 0.6377952755905512\n",
      "Fold 1 - QuadraticDiscriminantAnalysis -> 0.7139767504731008\n",
      "Fold 1 - ExtraTreesClassifier -> 0.6377952755905512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - RandomForestClassifier -> 0.531219980787704\n",
      "Fold 2 - GradientBoostingClassifier -> 0.600187265917603\n",
      "Fold 2 - QuadraticDiscriminantAnalysis -> 0.5060188224994528\n",
      "Fold 2 - ExtraTreesClassifier -> 0.5368601518491305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - RandomForestClassifier -> 0.590205432771882\n",
      "Fold 3 - GradientBoostingClassifier -> 0.5636623748211731\n",
      "Fold 3 - QuadraticDiscriminantAnalysis -> 0.6473308922721142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - ExtraTreesClassifier -> 0.5985693848354793\n",
      "Fold 4 - RandomForestClassifier -> 0.6524373699918604\n",
      "Fold 4 - GradientBoostingClassifier -> 0.5731969860064585\n",
      "Fold 4 - QuadraticDiscriminantAnalysis -> 0.4795311557355856\n",
      "Fold 4 - ExtraTreesClassifier -> 0.5278049648545244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - RandomForestClassifier -> 0.6353634237200034\n",
      "Fold 5 - GradientBoostingClassifier -> 0.6987784804643506\n",
      "Fold 5 - QuadraticDiscriminantAnalysis -> 0.6743772241992882\n",
      "Fold 5 - ExtraTreesClassifier -> 0.6856750257643421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RandomForestClassifier': np.float64(3.047021482862001),\n",
       " 'GradientBoostingClassifier': np.float64(3.0736203828001365),\n",
       " 'QuadraticDiscriminantAnalysis': np.float64(3.0212348451795417),\n",
       " 'ExtraTreesClassifier': np.float64(2.9867048028940273)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa_cross_val(models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa: 0.7226277372262774\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar SMOTE para balancear as classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Criar e treinar o modelo de Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calcular a métrica Kappa\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"Kappa: {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - RandomForestClassifier -> 0.6377952755905512\n",
      "Fold 1 - GradientBoostingClassifier -> 0.6377952755905512\n",
      "Fold 1 - QuadraticDiscriminantAnalysis -> 0.7139767504731008\n",
      "Fold 1 - ExtraTreesClassifier -> 0.6377952755905512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - RandomForestClassifier -> 0.531219980787704\n",
      "Fold 2 - GradientBoostingClassifier -> 0.600187265917603\n",
      "Fold 2 - QuadraticDiscriminantAnalysis -> 0.5060188224994528\n",
      "Fold 2 - ExtraTreesClassifier -> 0.5368601518491305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - RandomForestClassifier -> 0.590205432771882\n",
      "Fold 3 - GradientBoostingClassifier -> 0.5636623748211731\n",
      "Fold 3 - QuadraticDiscriminantAnalysis -> 0.6473308922721142\n",
      "Fold 3 - ExtraTreesClassifier -> 0.5985693848354793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - RandomForestClassifier -> 0.6524373699918604\n",
      "Fold 4 - GradientBoostingClassifier -> 0.5731969860064585\n",
      "Fold 4 - QuadraticDiscriminantAnalysis -> 0.4795311557355856\n",
      "Fold 4 - ExtraTreesClassifier -> 0.5278049648545244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - RandomForestClassifier -> 0.6353634237200034\n",
      "Fold 5 - GradientBoostingClassifier -> 0.6987784804643506\n",
      "Fold 5 - QuadraticDiscriminantAnalysis -> 0.6743772241992882\n",
      "Fold 5 - ExtraTreesClassifier -> 0.6856750257643421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauriciobenjamin700/.cache/pypoetry/virtualenvs/sistemas-inteligentes-tODIqpYk-py3.12/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "def kappa_smote_cross_val(models: list[RandomForestClassifier, GradientBoostingClassifier], X: pd.Series, y: pd.Series):\n",
    "    kappas = {}\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "        x_train, x_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "        y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "        for model in models:\n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "            y_pred = model.predict(x_test)\n",
    "            model_name = model.__class__.__name__\n",
    "            kappa = cohen_kappa_score(y_test, y_pred)\n",
    "            print(f\"Fold {i+1} - {model_name} -> {kappa}\")\n",
    "            kappas[model_name] = kappas.get(model_name, 0) + kappa\n",
    "\n",
    "    return kappas\n",
    "\n",
    "kappas = kappa_cross_val(models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor Kappa: 0.7021276595744681\n",
      "Melhores Hiperparâmetros: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar SMOTE para balancear as classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Definir o modelo e os hiperparâmetros para o Grid Search\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500, 1000,2000],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'max_depth': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Realizar o Grid Search com validação cruzada estratificada\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Melhor modelo encontrado pelo Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular a métrica Kappa\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"Melhor Kappa: {kappa}\")\n",
    "print(f\"Melhores Hiperparâmetros: {grid_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
